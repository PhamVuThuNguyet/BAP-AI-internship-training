{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Internship","language":"python","name":"internship"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"Bản sao của LSTM_Shakespeare.ipynb","provenance":[{"file_id":"https://github.com/PhamVuThuNguyet/BAP-Training_AI/blob/main/Neural%20Networks/RNN%20-%20LSTM/HWs/RNN_LSTM_2/HW2/LSTM_Shakespeare.ipynb","timestamp":1626676614067}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8cb15d82"},"source":["Given a character or sequence of characters, we want to predict the next character at each time step.\n","Model is trained to follow a language similar to the works of Shakespeare. The tinyshakespear dataset is used for training."],"id":"8cb15d82"},{"cell_type":"code","metadata":{"id":"88e449b9"},"source":["import numpy as np\n","import io\n","import re\n","import tensorflow as tf\n","import time\n","import os"],"id":"88e449b9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4127be92"},"source":["# Get data"],"id":"4127be92"},{"cell_type":"code","metadata":{"id":"b00a5916"},"source":["def read_text(URL):\n","    with io.open(URL, 'r', encoding='utf8') as f:\n","        text = f.read()\n","    # Character's collection\n","    return text"],"id":"b00a5916","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a0f45d38","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626668938404,"user_tz":-420,"elapsed":3,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"b41c94a7-0bbb-47b6-9869-ef22e2fe4a90"},"source":["# test\n","print(read_text(\"shakespeare_train.txt\")[:100])"],"id":"a0f45d38","execution_count":null,"outputs":[{"output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"758c16ad"},"source":["# Preprocessing"],"id":"758c16ad"},{"cell_type":"code","metadata":{"id":"1b189cd6"},"source":["#character to index\n","def character_to_index(sub_txt, dict_int):\n","    encoded_text = np.array([dict_int[c] for c in sub_txt], dtype=np.int32)  # encode data\n","    return encoded_text"],"id":"1b189cd6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c77066ae"},"source":["#index to character\n","def index_to_char(index_list, dict_char):\n","    text = []\n","    for i in index_list:\n","        text.append(dict_char[i])\n","    return (repr( ''.join(text)))"],"id":"c77066ae","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38290a31"},"source":["dict_int = {u:i for i, u in enumerate(read_text(\"shakespeare_train.txt\")[:100])}\n","dict_char = dict(enumerate(read_text(\"shakespeare_train.txt\")[:100]))"],"id":"38290a31","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"409d1378","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626668950594,"user_tz":-420,"elapsed":2,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"4323344e-f93e-43ad-f04f-2ac85a8a7a1a"},"source":["# test\n","print(\"Character to Index: \\n\")\n","for char,_ in zip(dict_int, range(65)):\n","    print('  {:4s}: {:3d}'.format(repr(char), dict_int[char]))"],"id":"409d1378","execution_count":null,"outputs":[{"output_type":"stream","text":["Character to Index: \n","\n","  'F' :  82\n","  'i' :  91\n","  'r' :  84\n","  's' :  85\n","  't' :  90\n","  ' ' :  87\n","  'C' :  88\n","  'z' :  92\n","  'e' :  93\n","  'n' :  94\n","  ':' :  95\n","  '\\n':  96\n","  'B' :  15\n","  'f' :  37\n","  'o' :  98\n","  'w' :  22\n","  'p' :  75\n","  'c' :  28\n","  'd' :  31\n","  'a' :  77\n","  'y' :  35\n","  'u' :  99\n","  'h' :  46\n","  ',' :  72\n","  'm' :  51\n","  'k' :  78\n","  '.' :  79\n","  'A' :  62\n","  'l' :  64\n","  'S' :  67\n","  'Y' :  97\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e5f20aac"},"source":["# Create training examples / targets"],"id":"e5f20aac"},{"cell_type":"markdown","metadata":{"id":"f45eaa6e"},"source":["Target value: for each sequence of characters, we return that sequence, shifted one position to the right, along with the new character that is predicted to follow the sequence.\n","\n","To create training examples of (input, target) pairs, we take the given sequence. The input is sequence with last word removed. Target is sequence with first word removed. Example: sequence: abc d ef input: abc d e target: bc d ef"],"id":"f45eaa6e"},{"cell_type":"code","metadata":{"id":"ec462e10"},"source":["def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text"],"id":"ec462e10","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6c1f498"},"source":["# Create training examples / targets\n","def handle_data(data, seq_len):\n","    \"\"\"\n","    this function to create data from row data\n","\n","    :param data: row data with int type\n","    :param seq_len: max len of input and output sequence\n","    :return: data for training\n","    \"\"\"\n","    #data4epoch = len(data) // (seq_len+1)\n","    # Create training examples / targets\n","    char_dataset = tf.data.Dataset.from_tensor_slices(data)\n","    sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)\n","    dataset = sequences.map(split_input_target)\n","    return dataset"],"id":"c6c1f498","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38dccf18"},"source":["training_set = read_text('shakespeare_train.txt')\n","val_set = read_text('shakespeare_valid.txt')"],"id":"38dccf18","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"abad86b8"},"source":["#union vocab\n","vocab_train = set(training_set)\n","vocab_val = set(val_set)\n","vocab = vocab_train.union(vocab_val)"],"id":"abad86b8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bec5592"},"source":["# set character that were found in text to the dict\n","dict_int = {u:i for i, u in enumerate(vocab)}\n","dict_char =dict(enumerate(vocab))\n","\n","train_x = character_to_index(training_set, dict_int)\n","val_x = character_to_index(val_set, dict_int)\n","\n","seq_len = 50 # max number of characters that can be fed as a single input\n","\n","#Create sequences from the individual characters. Our required size will be seq_len + 1 (character RNN)\n","train_seq = len(train_x) // (seq_len + 1)\n","val_seq = len(val_x) // (seq_len + 1)\n","\n","data_train = handle_data(train_x, seq_len) # include input and target\n","data_val = handle_data(val_x, seq_len)  # include input and target"],"id":"4bec5592","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1e4ce49"},"source":["# Build model"],"id":"a1e4ce49"},{"cell_type":"code","metadata":{"id":"85f0941d"},"source":["BATCH_SIZE = 64\n","iterator_train = train_seq // BATCH_SIZE\n","iterator_val = val_seq // BATCH_SIZE\n","\n","# Buffer used to shuffle the dataset\n","BUFFER_SIZE = train_seq + val_seq\n","data_train = data_train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","data_val = data_val.batch(BATCH_SIZE, drop_remainder=True)"],"id":"85f0941d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"112a47e0"},"source":["def built_model(cellType, vocab_size, embedding_dim, rnn_units, BATCH_SIZE):\n","    if (cellType == \"LSTM\"):\n","        rnn = tf.keras.layers.LSTM\n","    elif (cellType == \"GRU\"):\n","        rnn = tf.keras.layers.GRU\n","    else:\n","        rnn = tf.keras.layers.SimpleRNN\n","\n","    model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n","                                 rnn(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n","                                 tf.keras.layers.Dense(vocab_size)])\n","    return model "],"id":"112a47e0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"03e73544"},"source":["vocab_size = len(vocab)\n","embedding_dim = 256\n","rnn_units = 1024"],"id":"03e73544","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"422998ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626668976120,"user_tz":-420,"elapsed":784,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"834d269e-06ee-46c7-981e-5748b4db0b60"},"source":["cellType  = \"LSTM\" \n","model = built_model(cellType, vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n","print(model.summary())"],"id":"422998ff","execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (64, None, 256)           17152     \n","_________________________________________________________________\n","lstm (LSTM)                  (64, None, 1024)          5246976   \n","_________________________________________________________________\n","dense (Dense)                (64, None, 67)            68675     \n","=================================================================\n","Total params: 5,332,803\n","Trainable params: 5,332,803\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c2f3a1f9"},"source":["# Training model"],"id":"c2f3a1f9"},{"cell_type":"code","metadata":{"id":"334208ad"},"source":["def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"],"id":"334208ad","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"586a2b7f"},"source":["model.compile(optimizer='adam', loss=loss)"],"id":"586a2b7f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82450e10"},"source":["lstm_dir_checkpoints= 'training_checkpoints_LSTM'\n","checkpoint_prefix = os.path.join(lstm_dir_checkpoints, \"chkpt_{epoch}\")\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"],"id":"82450e10","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0a491e5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626675693108,"user_tz":-420,"elapsed":2356256,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"d55c02a4-372b-4426-e59c-14ac2185a9a6"},"source":["EPOCHS=100\n","history = model.fit(data_train, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"id":"f0a491e5","execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","1333/1333 [==============================] - 58s 34ms/step - loss: 1.9594\n","Epoch 2/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.5046\n","Epoch 3/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.4211\n","Epoch 4/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.3802\n","Epoch 5/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.3530\n","Epoch 6/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.3301\n","Epoch 7/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.3123\n","Epoch 8/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2959\n","Epoch 9/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2810\n","Epoch 10/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2672\n","Epoch 11/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2548\n","Epoch 12/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2430\n","Epoch 13/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2320\n","Epoch 14/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2218\n","Epoch 15/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2119\n","Epoch 16/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2026\n","Epoch 17/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1935\n","Epoch 18/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1854\n","Epoch 19/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1777\n","Epoch 20/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1708\n","Epoch 21/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1642\n","Epoch 22/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1574\n","Epoch 23/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1519\n","Epoch 24/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1471\n","Epoch 25/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1428\n","Epoch 26/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1383\n","Epoch 27/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1350\n","Epoch 28/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1313\n","Epoch 29/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1283\n","Epoch 30/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1253\n","Epoch 31/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1230\n","Epoch 32/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1212\n","Epoch 33/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1194\n","Epoch 34/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1174\n","Epoch 35/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1166\n","Epoch 36/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1160\n","Epoch 37/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1155\n","Epoch 38/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1142\n","Epoch 39/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1140\n","Epoch 40/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1141\n","Epoch 41/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1137\n","Epoch 42/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1136\n","Epoch 43/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1142\n","Epoch 44/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1138\n","Epoch 45/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1140\n","Epoch 46/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1156\n","Epoch 47/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1164\n","Epoch 48/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1173\n","Epoch 49/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1192\n","Epoch 50/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1199\n","Epoch 51/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1212\n","Epoch 52/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1222\n","Epoch 53/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1229\n","Epoch 54/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1252\n","Epoch 55/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1266\n","Epoch 56/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1283\n","Epoch 57/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1291\n","Epoch 58/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1313\n","Epoch 59/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1331\n","Epoch 60/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1353\n","Epoch 61/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1373\n","Epoch 62/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1393\n","Epoch 63/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1413\n","Epoch 64/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1440\n","Epoch 65/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1450\n","Epoch 66/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1471\n","Epoch 67/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1505\n","Epoch 68/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1529\n","Epoch 69/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1549\n","Epoch 70/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1587\n","Epoch 71/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1611\n","Epoch 72/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1638\n","Epoch 73/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1662\n","Epoch 74/100\n","1333/1333 [==============================] - 54s 37ms/step - loss: 1.1693\n","Epoch 75/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1722\n","Epoch 76/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1745\n","Epoch 77/100\n","1333/1333 [==============================] - 54s 37ms/step - loss: 1.1780\n","Epoch 78/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1821\n","Epoch 79/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1844\n","Epoch 80/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1875\n","Epoch 81/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1894\n","Epoch 82/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.1937\n","Epoch 83/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.1962\n","Epoch 84/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.1994\n","Epoch 85/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2029\n","Epoch 86/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2069\n","Epoch 87/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2083\n","Epoch 88/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2131\n","Epoch 89/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2161\n","Epoch 90/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2206\n","Epoch 91/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2218\n","Epoch 92/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2273\n","Epoch 93/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2299\n","Epoch 94/100\n","1333/1333 [==============================] - 52s 36ms/step - loss: 1.2333\n","Epoch 95/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2368\n","Epoch 96/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2393\n","Epoch 97/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2410\n","Epoch 98/100\n","1333/1333 [==============================] - 53s 36ms/step - loss: 1.2441\n","Epoch 99/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2487\n","Epoch 100/100\n","1333/1333 [==============================] - 53s 37ms/step - loss: 1.2525\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e63eb22e"},"source":["# Predicting"],"id":"e63eb22e"}]}